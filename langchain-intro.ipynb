{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will cover the very basics of LangChain. This includes chains and templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import necessary libraries, and use `dotenv` to load our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/wt0ch7xx1vbfcytgzjpnnsy40000gn/T/ipykernel_11943/1543617194.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=api_key)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every chat, LLMs are typically first introduced to a \"system message,\" instructing the LLM on how to interpret the conversation. There is also a \"human\" or \"user\" message which is simply what the user sends to the LLM. An \"assistant\" or \"AI\" message is associated with the messages that the LLM itself writes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/wt0ch7xx1vbfcytgzjpnnsy40000gn/T/ipykernel_11943/2497210350.py:6: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages=messages, temperature=0.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(1000, 10)\n",
      "y = np.random.randint(0, 2, size=(1000, 1))\n",
      "\n",
      "# Define the neural network\n",
      "model = keras.Sequential([\n",
      "    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
      "    keras.layers.Dense(64, activation='relu'),\n",
      "    keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam',\n",
      "              loss='binary_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist.\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data. Only return the script.\")\n",
    "]\n",
    "\n",
    "response = llm(messages=messages, temperature=0.6)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of the \"temperature\" parameter. Temperature is a parameter that refers to the probability with which the LLM's underlying next token predictor picks out a next token that is not the highest probability token. You can consider to it a proxy for \"creativity.\" Higher temperature -> more randomness -> more \"creativity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent prompts using \"prompt templates,\" which allow us to dynamically plug things into prompts. The `PromptTemplate` class is simply LangChain's object interface with prompts. Chains allow us to link prompts together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain {topic} in one sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/wt0ch7xx1vbfcytgzjpnnsy40000gn/T/ipykernel_11943/1723417862.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm)\n",
      "/var/folders/tl/wt0ch7xx1vbfcytgzjpnnsy40000gn/T/ipykernel_11943/1723417862.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(topic=\"atitude\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Attitude refers to a person's overall outlook or disposition towards a particular situation, person, or event.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "chain.run(topic=\"atitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Potato is a starchy tuber vegetable that is a staple food in many cuisines around the world.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_topic_desc\"],\n",
    "    template=\"\"\"\n",
    "    You are a:\n",
    "    {ml_topic_desc}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "chain.run(topic = \"potato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the first chain are passed in to the second chain as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAn autoencoder is a type of neural network that learns to encode input data into a lower-dimensional representation and then decode it back to its original form.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Autoencoders are commonly used for tasks such as data compression, denoising, and dimensionality reduction. They consist of an encoder and a decoder, which are typically implemented as neural networks. The encoder takes the input data and generates a compressed representation (also known as a latent space representation) of the data. The decoder then takes this compressed representation and reconstructs the original input data.\n",
      "\n",
      "Autoencoders are unsupervised learning models, meaning they do not require labeled data for training. Instead, they learn to reconstruct the input data by minimizing a reconstruction error, such as mean squared error or binary cross-entropy loss.\n",
      "\n",
      "Autoencoders have been used in various applications, such as image and video processing, anomaly detection, and feature learning. They can also be used for generating new data samples by sampling from the learned latent space representation.\n",
      "\n",
      "Overall, autoencoders are a powerful tool in the field of machine learning and have been successfully applied in a wide range of domains.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Autoencoders are commonly used for tasks such as data compression, denoising, and dimensionality reduction. They consist of an encoder and a decoder, which are typically implemented as neural networks. The encoder takes the input data and generates a compressed representation (also known as a latent space representation) of the data. The decoder then takes this compressed representation and reconstructs the original input data.\n",
      "\n",
      "Autoencoders are unsupervised learning models, meaning they do not require labeled data for training. Instead, they learn to reconstruct the input data by minimizing a reconstruction error, such as mean squared error or binary cross-entropy loss.\n",
      "\n",
      "Autoencoders have been used in various applications, such as image and video processing, anomaly detection, and feature learning. They can also be used for generating new data samples by sampling from the learned latent space representation.\n",
      "\n",
      "Overall, autoencoders are a powerful tool in the field of machine learning and have been successfully applied in a wide range of domains.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "blog_post = overall_chain.run(input=\"autoencoder\")\n",
    "print(blog_post)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
